{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'In': ['', u'vars()'],\n",
       " 'Out': {},\n",
       " 'SQLContext': pyspark.sql.context.SQLContext,\n",
       " 'SparkContext': pyspark.context.SparkContext,\n",
       " 'SparkSession': pyspark.sql.session.SparkSession,\n",
       " 'StorageLevel': pyspark.storagelevel.StorageLevel,\n",
       " '_': '',\n",
       " '__': '',\n",
       " '___': '',\n",
       " '__builtin__': <module '__builtin__' (built-in)>,\n",
       " '__builtins__': <module '__builtin__' (built-in)>,\n",
       " '__doc__': '\\nAn interactive shell.\\n\\nThis file is designed to be launched as a PYTHONSTARTUP script.\\n',\n",
       " '__name__': '__main__',\n",
       " '__package__': None,\n",
       " '_dh': [u'/home/gpadmin/spark'],\n",
       " '_i': u'',\n",
       " '_i1': u'vars()',\n",
       " '_ih': ['', u'vars()'],\n",
       " '_ii': u'',\n",
       " '_iii': u'',\n",
       " '_oh': {},\n",
       " '_pythonstartup': '',\n",
       " '_sh': <module 'IPython.core.shadowns' from '/opt/anaconda/lib/python2.7/site-packages/IPython/core/shadowns.pyc'>,\n",
       " 'atexit': <module 'atexit' from '/opt/anaconda/lib/python2.7/atexit.pyc'>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f72ddfa7310>,\n",
       " 'get_ipython': <bound method ZMQInteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f72de013a50>>,\n",
       " 'os': <module 'os' from '/opt/anaconda/lib/python2.7/os.pyc'>,\n",
       " 'platform': <module 'platform' from '/opt/anaconda/lib/python2.7/platform.pyc'>,\n",
       " 'py4j': <module 'py4j' from '/opt/anaconda/lib/python2.7/site-packages/py4j/__init__.pyc'>,\n",
       " 'pyspark': <module 'pyspark' from '/home/gpadmin/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/__init__.pyc'>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f72ddfa7310>,\n",
       " 'sc': <pyspark.context.SparkContext at 0x7f72d85adfd0>,\n",
       " 'spark': <pyspark.sql.session.SparkSession at 0x7f72c0e23e50>,\n",
       " 'sqlContext': <pyspark.sql.context.SQLContext at 0x7f72c0dae710>,\n",
       " 'sqlCtx': <pyspark.sql.context.SQLContext at 0x7f72c0dae710>}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.driver.host', u'10.0.0.10'),\n",
       " (u'spark.executor.extraClassPath', u'/usr/share/java/postgresql-jdbc.jar'),\n",
       " (u'spark.driver.port', u'35222'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.app.id', u'local-1480405447043'),\n",
       " (u'spark.master', u'local[*]'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.driver.extraClassPath', u'/usr/share/java/postgresql-jdbc.jar'),\n",
       " (u'spark.app.name', u'PySparkShell'),\n",
       " (u'hive.metastore.warehouse.dir',\n",
       "  u'file:/home/gpadmin/spark/spark-warehouse')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# echo \"export SPARK_CLASSPATH=/usr/share/java/postgresql-jdbc.jar\" >> ${SPARK_HOME}/bin/load-spark-env.sh\n",
    "\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query data from the master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 31|\n",
      "| 63|\n",
      "| 95|\n",
      "|127|\n",
      "|159|\n",
      "|191|\n",
      "|223|\n",
      "|255|\n",
      "|260|\n",
      "|292|\n",
      "|324|\n",
      "|356|\n",
      "|388|\n",
      "|420|\n",
      "|452|\n",
      "|484|\n",
      "|521|\n",
      "|553|\n",
      "|585|\n",
      "|617|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrameReader\n",
    "\n",
    "# Greenplum master\n",
    "url = 'postgresql://gpdb-mdw:5432/postgres'\n",
    "\n",
    "properties = {'user': 'gpadmin', 'password': '', 'driver': 'org.postgresql.Driver', 'PGOPTIONS': 'gp_session_role=utility'}\n",
    "df = DataFrameReader(sqlContext).jdbc(\n",
    "  url='jdbc:%s' % url, table='sparktest', properties=properties\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to query data in parallel from the segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import generators    # needs to be at the top of your module\n",
    "\n",
    "recordBatchSize = 100000\n",
    "\n",
    "def ResultIter(cursor, arraysize):\n",
    "    'An iterator that uses fetchmany to keep memory usage down'\n",
    "    while True:\n",
    "        results = cursor.fetchone() #many(arraysize)\n",
    "        if not results:\n",
    "            break\n",
    "        for result in results:\n",
    "            yield result\n",
    "\n",
    "def fetch(host, port, database, query):\n",
    "    from os import environ\n",
    "    import psycopg2\n",
    "\n",
    "    # Allow connections directly to the segments\n",
    "    environ['PGOPTIONS'] = '-c gp_session_role=utility'\n",
    "\n",
    "    cnn = psycopg2.connect(host=host, port=port, database=database, user=\"gpadmin\")\n",
    "    cur = cnn.cursor(\"serverSideCursor\")\n",
    "    cur.itersize = recordBatchSize\n",
    "    cur.execute(\"{0}\".format(query))\n",
    "    \n",
    "    return cur.fetchall()\n",
    "    \n",
    "    #return ResultIter(cur, recordBatchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a list of segment host + port tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numNodes = 4\n",
    "hosts = [\"sdw{0}\".format(h) for h in xrange(1, numNodes + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numSegsPerNode = 8\n",
    "ports = [p for p in xrange(40000, 40000+numSegsPerNode)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segmentProcesses = [(h, p) for h in hosts for p in ports ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an RDD containing the hostname, port tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(segmentProcesses, numNodes*numSegsPerNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the segments in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segData = rdd.map(lambda seg: fetch(seg[0], seg[1], \"plsprod\", \"select * from problems_reduced\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One list per segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "segData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all segment lists into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879847029\n",
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "allData = segData.flatMap(lambda xs: chain(*xs))\n",
    "print allData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "#allData = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = allData.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
